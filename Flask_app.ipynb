{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzkfjSgFtlKY",
        "outputId": "f5111652-9050-468c-afd4-9d3f8e90bb4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok==4.1.1\n",
            "  Downloading pyngrok-4.1.1.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyngrok==4.1.1) (0.18.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok==4.1.1) (6.0.1)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-4.1.1-py3-none-any.whl size=15963 sha256=274f56629f1c0546eed7133475b9fa95e1b0acd3c0704d1b297bb1bbd24b4dad\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/7c/4c/632fba2ea8e88d8890102eb07bc922e1ca8fa14db5902c91a8\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-4.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok==4.1.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TW0a3uXEiwy",
        "outputId": "b06416bb-7e6b-4024-e773-cb794f7fe813"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flask_ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask_ngrok) (2.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask_ngrok) (2.31.0)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok) (3.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.8->flask_ngrok) (2.1.3)\n",
            "Installing collected packages: flask_ngrok\n",
            "Successfully installed flask_ngrok-0.0.25\n"
          ]
        }
      ],
      "source": [
        "!pip install flask_ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faYzZz6FLqHc",
        "outputId": "7b3269fe-ef4b-4b01-f37b-ade7d9f0a9ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/126.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/126.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2023.7.22)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install vaderSentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tItMNSSnLk48",
        "outputId": "9bff8465-7dd7-4f9a-f83a-63b366aa89cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "That somehow managed to be record short yet answer almost all the questions we would've asked, haha! Hi Deb! Welcome to Hou Tian; nice to meet you! I'm Jhenne, one of the mods here-- which means I gotta give you the modly speech :] Make sure to check out the Mandatory Reading up top! Our constantly updated Library is also a great resource, though it isn't mandatory reading-- we like to tell members to 'read as you need', rather than marathon read it all at once. One of the most helpful threads is the Gameplay So Far thread, which breaks down what all has gone down on the boards. (Now, the summary for January isn't tossed up yet, but we'd be happy to break down what you missed if you'd like.) I see that you're interested in Mai! That means both the Trying for a Canon Character page, and the Canon Character Rules and Consequences post will be helpful to check out. If you're ever considering an original character, we have our player-made adoptables list, and our factions, comprised of the Jade Shark/Bending Opposition, Original People of the Flame, and The Bending Crime Syndicates. As far as characters go, in the past tense I play Srai, a Jade Shark [s]that is very very dusty. In the Korraverse I play a reporter named Chihiro, and an ex-taxi dancer/wannabe actress named Naoki, and a Republic City University student named Haruna. I think that's it! If you have any questions, don't hesitate to ask a mod, or drop it right here in this thread so we can get back to you! Again, welcome! #CONFETTI\n",
            "                                         clean_posts  compound_sentiment  \\\n",
            "0  hat somehow managed record short yet answer al...              0.9834   \n",
            "\n",
            "   ADJ_avg  ADP_avg  ADV_avg  CONJ_avg  DET_avg  NOUN_avg  NUM_avg  PRT_avg  \\\n",
            "0       27       10       19        29       30        66        1        6   \n",
            "\n",
            "   ...  qm  em  colons  emojis  word_count  unique_words  upper  link_count  \\\n",
            "0  ...   0   8       1       0         271           180      6           0   \n",
            "\n",
            "   ellipses  img_count  \n",
            "0         0          0  \n",
            "\n",
            "[1 rows x 22 columns]\n",
            "Preprocessing Time: 1.6714370250701904 seconds\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import time\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from nltk import wordnet\n",
        "#from nltk import averaged_perceptron_tagger\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# 16 MBTI types\n",
        "mbti = [\n",
        "    \"INFP\",\n",
        "    \"INFJ\",\n",
        "    \"INTP\",\n",
        "    \"INTJ\",\n",
        "    \"ENTP\",\n",
        "    \"enfp\",\n",
        "    \"ISTP\",\n",
        "    \"ISFP\",\n",
        "    \"ENTJ\",\n",
        "    \"ISTJ\",\n",
        "    \"ENFJ\",\n",
        "    \"ISFJ\",\n",
        "    \"ESTP\",\n",
        "    \"ESFP\",\n",
        "    \"ESFJ\",\n",
        "    \"ESTJ\",\n",
        "]\n",
        "\n",
        "\n",
        "tags_dict = {\n",
        "    \"ADJ_avg\": [\"JJ\", \"JJR\", \"JJS\"],\n",
        "    \"ADP_avg\": [\"EX\", \"TO\"],\n",
        "    \"ADV_avg\": [\"RB\", \"RBR\", \"RBS\", \"WRB\"],\n",
        "    \"CONJ_avg\": [\"CC\", \"IN\"],\n",
        "    \"DET_avg\": [\"DT\", \"PDT\", \"WDT\"],\n",
        "    \"NOUN_avg\": [\"NN\", \"NNS\", \"NNP\", \"NNPS\"],\n",
        "    \"NUM_avg\": [\"CD\"],\n",
        "    \"PRT_avg\": [\"RP\"],\n",
        "    \"PRON_avg\": [\"PRP\", \"PRP$\", \"WP\", \"WP$\"],\n",
        "    \"VERB_avg\": [\"MD\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"],\n",
        "    \".\": [\"#\", \"$\", \"''\", \"(\", \")\", \",\", \".\", \":\"],\n",
        "    \"X\": [\"FW\", \"LS\", \"UH\"],\n",
        "}\n",
        "\n",
        "features = [\n",
        "    \"clean_posts\",\n",
        "    \"compound_sentiment\",\n",
        "    \"ADJ_avg\",\n",
        "    \"ADP_avg\",\n",
        "    \"ADV_avg\",\n",
        "    \"CONJ_avg\",\n",
        "    \"DET_avg\",\n",
        "    \"NOUN_avg\",\n",
        "    \"NUM_avg\",\n",
        "    \"PRT_avg\",\n",
        "    \"PRON_avg\",\n",
        "    \"VERB_avg\",\n",
        "    \"qm\",\n",
        "    \"em\",\n",
        "    \"colons\",\n",
        "    \"emojis\",\n",
        "    \"word_count\",\n",
        "    \"unique_words\",\n",
        "    \"upper\",\n",
        "    \"link_count\",\n",
        "    \"ellipses\",\n",
        "    \"img_count\",\n",
        "]\n",
        "\n",
        "\n",
        "def unique_words(s):\n",
        "    unique = set(s.split(\" \"))\n",
        "    return len(unique)\n",
        "\n",
        "\n",
        "def emojis(post):\n",
        "\n",
        "    emoji_count = 0\n",
        "    words = post.split()\n",
        "    for e in words:\n",
        "        if \"http\" not in e:\n",
        "            if e.count(\":\") == 2:\n",
        "                emoji_count += 1\n",
        "    return emoji_count\n",
        "\n",
        "\n",
        "def colons(post):\n",
        "\n",
        "    colon_count = 0\n",
        "    words = post.split()\n",
        "    for e in words:\n",
        "        if \"http\" not in e:\n",
        "            colon_count += e.count(\":\")\n",
        "    return colon_count\n",
        "\n",
        "\n",
        "def lemmitize(s):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    new_s = \"\"\n",
        "    for word in s.split(\" \"):\n",
        "        lemmatizer.lemmatize(word)\n",
        "        if word not in stopwords.words(\"english\"):\n",
        "            new_s += word + \" \"\n",
        "    return new_s[:-1]\n",
        "\n",
        "\n",
        "def clean(s):\n",
        "\n",
        "    s = re.sub(re.compile(r\"https?:\\/\\/(www)?.?([A-Za-z_0-9-]+).*\"), \"\", s)\n",
        "\n",
        "    s = re.sub(re.compile(r\"\\S+@\\S+\"), \"\", s)\n",
        "\n",
        "    s = re.sub(re.compile(r\"[^a-z\\s]\"), \"\", s)\n",
        "\n",
        "    s = s.lower()\n",
        "\n",
        "    for type_word in mbti:\n",
        "        s = s.replace(type_word.lower(), \"\")\n",
        "    return s\n",
        "\n",
        "\n",
        "def prep_counts(s):\n",
        "    clean_s = clean(s)\n",
        "    d = {\n",
        "        \"clean_posts\": lemmitize(clean_s),\n",
        "        \"link_count\": s.count(\"http\"),\n",
        "        \"youtube\": s.count(\"youtube\") + s.count(\"youtu.be\"),\n",
        "        \"img_count\": len(re.findall(r\"(\\.jpg)|(\\.jpeg)|(\\.gif)|(\\.png)\", s)),\n",
        "        \"upper\": len([x for x in s.split() if x.isupper()]),\n",
        "        \"char_count\": len(s),\n",
        "        \"word_count\": clean_s.count(\" \") + 1,\n",
        "        \"qm\": s.count(\"?\"),\n",
        "        \"em\": s.count(\"!\"),\n",
        "        \"colons\": colons(s),\n",
        "        \"emojis\": emojis(s),\n",
        "        \"unique_words\": unique_words(clean_s),\n",
        "        \"ellipses\": len(re.findall(r\"\\.\\.\\.\\ \", s)),\n",
        "    }\n",
        "    return clean_s, d\n",
        "\n",
        "\n",
        "def prep_sentiment(s):\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    score = analyzer.polarity_scores(s)\n",
        "    d = {\n",
        "        \"compound_sentiment\": score[\"compound\"],\n",
        "        \"pos_sentiment\": score[\"pos\"],\n",
        "        \"neg_sentiment\": score[\"neg\"],\n",
        "        \"neu_sentiment\": score[\"neu\"],\n",
        "    }\n",
        "    return d\n",
        "\n",
        "\n",
        "def tag_pos(s):\n",
        "    tagged_words = nltk.pos_tag(word_tokenize(s))\n",
        "    d = dict.fromkeys(tags_dict, 0)\n",
        "    for tup in tagged_words:\n",
        "        tag = tup[1]\n",
        "        for key, val in tags_dict.items():\n",
        "            if tag in val:\n",
        "                tag = key\n",
        "        d[tag] += 1\n",
        "    return d\n",
        "\n",
        "\n",
        "def prep_data(s):\n",
        "    clean_s, d = prep_counts(s)\n",
        "    d.update(prep_sentiment(lemmitize(clean_s)))\n",
        "    d.update(tag_pos(clean_s))\n",
        "    return pd.DataFrame([d])[features]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    t = time.time()\n",
        "    string = \"That somehow managed to be record short yet answer almost all the questions we would've asked, haha! Hi Deb! Welcome to Hou Tian; nice to meet you! I'm Jhenne, one of the mods here-- which means I gotta give you the modly speech :] Make sure to check out the Mandatory Reading up top! Our constantly updated Library is also a great resource, though it isn't mandatory reading-- we like to tell members to 'read as you need', rather than marathon read it all at once. One of the most helpful threads is the Gameplay So Far thread, which breaks down what all has gone down on the boards. (Now, the summary for January isn't tossed up yet, but we'd be happy to break down what you missed if you'd like.) I see that you're interested in Mai! That means both the Trying for a Canon Character page, and the Canon Character Rules and Consequences post will be helpful to check out. If you're ever considering an original character, we have our player-made adoptables list, and our factions, comprised of the Jade Shark/Bending Opposition, Original People of the Flame, and The Bending Crime Syndicates. As far as characters go, in the past tense I play Srai, a Jade Shark [s]that is very very dusty. In the Korraverse I play a reporter named Chihiro, and an ex-taxi dancer/wannabe actress named Naoki, and a Republic City University student named Haruna. I think that's it! If you have any questions, don't hesitate to ask a mod, or drop it right here in this thread so we can get back to you! Again, welcome! #CONFETTI\"\n",
        "    print(string)\n",
        "    print(prep_data(string))\n",
        "    print(f\"Preprocessing Time: {time.time() - t} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "print(sklearn.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXQ08pdu1Vis",
        "outputId": "32876e4d-0c12-4bb1-a847-f8c3e78d7ea1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3uUtouALk7-",
        "outputId": "6978f0f7-3a23-4a19-e952-08bb9f37e653"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I love everyone!!!!!!\n",
            "ENFP\n",
            "Preprocessing Time: 0.9136931896209717 seconds\n"
          ]
        }
      ],
      "source": [
        "from joblib import load\n",
        "#from preprocess import prep_data\n",
        "import time\n",
        "import os\n",
        "\n",
        "def trace_back(combined):\n",
        "    type_list = [\n",
        "        {\"0\": \"I\", \"1\": \"E\"},\n",
        "        {\"0\": \"N\", \"1\": \"S\"},\n",
        "        {\"0\": \"F\", \"1\": \"T\"},\n",
        "        {\"0\": \"P\", \"1\": \"J\"},\n",
        "    ]\n",
        "    result = []\n",
        "    for num in combined:\n",
        "        s = \"\"\n",
        "        for i in range(len(num)):\n",
        "            s += type_list[i][num[i]]\n",
        "        result.append(s)\n",
        "    return result\n",
        "\n",
        "\n",
        "def combine_classes(y_pred1, y_pred2, y_pred3, y_pred4):\n",
        "    combined = []\n",
        "    for i in range(len(y_pred1)):\n",
        "        combined.append(\n",
        "            str(y_pred1[i]) + str(y_pred2[i]) + str(y_pred3[i]) + str(y_pred4[i])\n",
        "        )\n",
        "    result = trace_back(combined)\n",
        "    return result[0]\n",
        "\n",
        "\n",
        "\n",
        "def predict(s):\n",
        "\n",
        "    X = prep_data(s)\n",
        "\n",
        "    # loading the 4 models\n",
        "    EorI_model = load(os.path.join('/content/clf_is_extrovert.joblib'))\n",
        "    SorN_model = load(os.path.join(\"/content/clf_is_Sensing.joblib\"))\n",
        "    TorF_model = load(os.path.join(\"/content/clf_is_Thinking.joblib\"))\n",
        "    JorP_model = load(os.path.join(\"/content/clf_is_Judging.joblib\"))\n",
        "\n",
        "    # predicting\n",
        "    EorI_pred = EorI_model.predict(X)\n",
        "    SorN_pred = SorN_model.predict(X)\n",
        "    TorF_pred = TorF_model.predict(X)\n",
        "    JorP_pred = JorP_model.predict(X)\n",
        "\n",
        "    # combining the predictions from the 4 models\n",
        "    result = combine_classes(EorI_pred, SorN_pred, TorF_pred, JorP_pred)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    t = time.time()\n",
        "    string = \"I love everyone!!!!!!\"\n",
        "    print(string)\n",
        "    print(predict(string))\n",
        "    print(f\"Preprocessing Time: {time.time() - t} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2F73G-dFtvdB"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "from flask_ngrok import run_with_ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5D5kVsdlJDc",
        "outputId": "7c958dce-8009-4e8e-d37a-0967a2950b01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "source": [
        "ngrok.set_auth_token(\"2T4NL6lQCTj3BiwuOed8U8c6VtY_X2SZDT5UirFiGFZANe8J\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gynt8A2Xt0iC"
      },
      "outputs": [],
      "source": [
        "port_no=5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bMU6_Pq4DlLc"
      },
      "outputs": [],
      "source": [
        "template_folder='/content/drive/MyDrive/template'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xk83YR0Z4xUV"
      },
      "outputs": [],
      "source": [
        "!cd /content/drive/MyDrive/template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Cg3P-02unox",
        "outputId": "afdcdcbc-417f-404a-bf00-7ed9b99bb325"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Running on http://95ee-34-23-63-128.ngrok-free.app\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [19/Oct/2023 13:51:05] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [19/Oct/2023 13:51:06] \"\u001b[33mGET /static/css/stylesheet.css HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [19/Oct/2023 13:51:06] \"\u001b[33mGET /static/js/script.js HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [19/Oct/2023 13:51:10] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [19/Oct/2023 13:51:29] \"GET /methodology HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [19/Oct/2023 13:51:30] \"\u001b[33mGET /static/css/stylesheet.css HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [19/Oct/2023 13:51:30] \"\u001b[33mGET /static/images/type_hist.png HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [19/Oct/2023 13:51:30] \"\u001b[33mGET /static/js/script.js HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [19/Oct/2023 13:51:30] \"\u001b[33mGET /static/images/correlation.png HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [19/Oct/2023 13:51:30] \"\u001b[33mGET /static/images/class_imbalance_2.png HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [19/Oct/2023 13:51:36] \"GET /analysis HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [19/Oct/2023 13:51:36] \"\u001b[33mGET /static/css/stylesheet.css HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [19/Oct/2023 13:51:36] \"\u001b[33mGET /static/js/script.js HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [19/Oct/2023 13:52:01] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [19/Oct/2023 13:52:02] \"\u001b[33mGET /static/css/stylesheet.css HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [19/Oct/2023 13:52:02] \"\u001b[33mGET /static/js/script.js HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [19/Oct/2023 13:53:15] \"POST /response HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [19/Oct/2023 13:53:16] \"\u001b[33mGET /static/css/stylesheet.css HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [19/Oct/2023 13:53:16] \"\u001b[33mGET /static/js/script.js HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [19/Oct/2023 13:53:33] \"\u001b[33mGET /static/js/script.js HTTP/1.1\u001b[0m\" 404 -\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask,render_template,request\n",
        "app=Flask(__name__,template_folder=template_folder)\n",
        "import requests\n",
        "#ngrok.set_auth_token(\"2T4NL6lQCTj3BiwuOed8U8c6VtY_X2SZDT5UirFiGFZANe8J\")\n",
        "#public_url=ngrok.connect(5000).public_url\n",
        "run_with_ngrok(app)\n",
        "@app.route('/')\n",
        "@app.route('/home')\n",
        "def home():\n",
        "  return render_template('index.html')\n",
        "@app.route(\"/response\", methods=[\"GET\", \"POST\"])\n",
        "def response():\n",
        "    if request.method == \"POST\":\n",
        "        snippet = request.form[\"fsnippet\"]\n",
        "        personality_type = predict(snippet)\n",
        "    return render_template(\"response.html\", name=personality_type, string=snippet)\n",
        "@app.route('/analysis')\n",
        "def analysis():\n",
        "  return render_template('analysis.html')\n",
        "@app.route('/methodology')\n",
        "def methodology():\n",
        "  return render_template('methodology.html')\n",
        "\n",
        "app.run()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}